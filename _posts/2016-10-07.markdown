---
layout: post
title:  "Algorithmic Trading Defined"
date:   2016-10-08 19:17:00
author: Ryan Castner
categories: Empyre
tags: empyre quant quantitative finance math
cover:  "/assets/.jpg"
---

To gain a fundamental understanding of something we must first be able to describe it. Then we must know its history. I believe every Computer Science course I have taken and most courses of note will have a historical component. The history of a topic, of a field of study acknowledges the work of the pioneers in the field. History is important because it informs us about whose shoulders we are standing on when we set out on our journey.

## Advantages of Algorithmic Trading

Algorithmic trading is the use of computers and a deterministic set of instructions to automate the process of financial trading. The predecessor and alternative to algorithmic trading is discretionary trading, where a human makes trading decisions and executes them. There are numerous benefits to the algorithmic trading approach over the discretionary one.

Discretionary trading requires a person to be constantly vigilant on any input that might affect trading strategies of their portfolio. Often times in larger companies this would be a team of people constantly monitoring news sources and the markets to make decisions. The human brain has too many biases to make consistently profitable decisions. In Nassim Taleb's 'The Black Swan', Taleb describes many cognitive biases that occur that can aversely affect trading outcomes and increase risk. For instance, the sunken ship fallacy states that if we invest in a failing stock, we are likely to stick with it hoping for a rebound rather than selling. The reason this happens is because to sell would be to admit our intuitions were incorrect and so we stay the course hoping we will reach out destination while the ship continues to sink.

There is the argument that experts though can get past these biases and make informed decisions, that a human expert in trading could be more knowledgeable than a machine. Taleb debunked this expert fallacy as well, citing study after study where experts have little to no improved performance over a lay person when it comes to accuracy of prediction. There are people who get lucky, though. That luck is the premise of Taleb's book, *The Black Swan* which refers to the fact that no one believe a black swan could exist until one day we saw it happen in nature, a freak occurrence with an infintesimally small probability. Luck exists, and Taleb posits that traders, CEO's and others who have made it big on discretionary trading did so through Black Swan events, and once they got the boost they were able to for the most part stay on top. He analyzed a trading firms top employees performances over a decade and found that there were as many bad years as good ones on average and there was no consistency.

Algorithmic trading eliminates the human element. There are no biases, no gut feelings or intuitions. There is still luck, we didn't lose that upside, we just eliminated extra variables, inputs, or sources of risk from the equation. The name of the game is consistency, if on average an algorithm can trade at a profit, with no work on the part of the owner they will continue to earn. Time is then freed up to diversify their portfolio, implement new trading strategies, tweak existing ones, or even go on vacation. We get another benefit from eliminating human decision making from the process, speed and frequency. A computer program can execute millions of instructions per second and do far more far faster than any human mind could ever hope to. A computer program can also trade faster. When news breaks about a big event it causes a reverberation though the markets. A machine can pick up on this discord instantly and informatively act on it, any discrentionary trader who does so will have already missed the boat. So more speed and higher number of trades is just another feather in the cap of algorithmic trading.

## Disadvantages of Algorithmic Trading

Now that we have built up algorithmic trading to sound like the godsend of the financial world, let us knock it down a few pegs with some realities.

- One needs to know how to create a profitiable algorithm and run the system around it
- One needs higher capital to invest than a discretionary trader would

The first one is obvious but it is not something to flippantly dismiss. Programming is difficult. Not only are you creating a working progrommatic system and algorithm, you need to make one that beats out your competitors who also are running their own systems. This requires a lot of scientific and mathematical knowledge to create abstracted models that can represent markets.

The second point is important as well. There are a number of costs associated with algorithmic trading that do not exist with discrentionary trading. First off there are costs associated with running the system. You need a server with enough hardware processing power and memory to effectively compete. The server needs to be as close to your data sources as possible to reduce latency as speed is the name of the game. This means you likely have to rent rack space from a company for a monthly fee, this could run you anywhere from $5-$100 a month or more depending on how beefed out you want your hardware. Algorithmic trading requires data inputs, there are free resources but they do not update as fast. Subscriptions to streams of intraday data can run $300-$500 a month for the common ones, and in the $10,000's for commercial, specialized ones. Furthermore you need initial capital to invest, most services that allow automated trades require account balances of $10,000+ and require a certain number of trades per month. Hitting the number of trades per month might not seem like a big deal, but there are costs associated with each trade too.

## Going about exploring Algorithmic Trading, or why this is not data mining

To those who have a computer science background, it may seem obvious that the trading algorithms developed are best suited for data mining strategies. While it is possible to use data mining: gathering of attributes, analyzing which ones best predict performance, and applying well known supervised and unsupervised algorithms to them to create prediction models for trades, it is not the way I will be pursuing this topic. The reason is that data mining is often a black box approach, while a human might use their intuition during the process, feature selection is usually done by statistical analysis. The researcher won't know why these features are the best, they just know that statistically they performed the best, and that a certain model performed the best. They can potentially overfit their models during backtesting and the rollout of the strategy, which might even be initially profitable, will fail, and there will be no known reason why. At this point the researcher commonly starts the process over again, taking in new data or updated data to create a new model. This time consuming and blinded approach to trading removes one of the great powers of humans, to ask questions.

The approach we will be using instead is based on the Scientific method. A hypothesis is formed to explain some observable behavior. It is our job to then prove or disprove the null hypothesis. The null hypothesis states that the hypothesis's independent variable has no impact or bearing on the dependent variable in any meaningful way. That is to say, any perceived impact is the result of pure randomness. The use of p-values is a common statistical method to prove significance which we will discuss later. For now suffice it to say that it gives us a confidence level that the null hypothesis is true or false (depending on how you look at the number).

AS a short aside after discrediting the data mining approach, often times now neural networks are used to automate the renewing of the research process. Neural networks learn and adapt overtime to changes, so if an indicator or feature falls out of favor it learns to weigh it less and weigh other indicators better. In this way it is constantly 'learning' and adapting to conditions and could be considered a superiour method. Neural networks are complex and hard to get correct. They take a lot of time to train and need a lot of good data to be trained properly. For these reasons we will stick with the scientific approach.

## Empyre's Approach and Goals

Empyre has aspirations to be a robust trading platform, that, given I or potential investors can meet the required capital conditions, could utilize to turn a profit. For the time being though, Empyre will be academic in nature. The website will simulate and showcase trading strategies by pitting them against a stream of historical data. When an algorithm has a stream of historical data run past it to evaluate performance it is called **backtesting**.

Backtesting allows us to view the performance of the algorithm as it produces trading signals which will accumulate to net losses or gains. It is important to note that algorithms can easily be overfitted. Overfitting is where an algorithm is tuned to a particular model so well that it does not generalize well to other models. Meaning, the algorithm performs great with one specific run of data but when run against other streams it performs poorly. We have to be careful to not overfit, and we will borrow from data mining to help prevent it. In data mining it is common to set aside a section of your training data to validate against. A common method is called n-folds cross-validation where *n* slices of training data are taken, the model is trained against one slice and then evaluated against the other slices to *cross-validate* the results and ensure they generalize.

By utilizing backtesting we can eliminate non-performant strategies, optimize performing ones, and model them against past data in a way that requires no trading costs, initial balances, or other fees. Empyre at first will be a proof-of-concept, the usage of CS principles, web design principles, server administration, mathematics, and database management systems to solve a complex problem.